---
title: Introduction to Gaussian Processes
author: Govind G Nair
date: '2019-08-11'
slug: introduction-to-gaussian-processes
categories:
  - Stats
  - Machine Learning
tags:
  - Machine Learning
---



<div id="gaussian-distributions" class="section level2">
<h2>Gaussian Distributions</h2>
<p>Fundamental to understanding a Gaussian process is a Gaussian distribution.The 1 dimensional version of the Gaussian distribution is the Normal distribution or the Bell Curve.</p>
<p>The plot of a standard normal distribution with mean 0 and variance 1 is shown below. Note that the curve doesn’t look like a perfect bell as the data has been simulated</p>
<pre class="r"><code>set.seed(0)
plot(density(rnorm(100000)),main = bquote(&quot;N(&quot;~mu~&quot;=0,&quot;~sigma^2~&quot;=1)&quot;))</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>A bi-variate Gaussian distribution is simply a Gaussian distribution in 2 dimensions. Whereas a uni-variate Gaussian distribution is parameterized by two scalar quantities (i.e. the mean and variance),the bi-variate Gaussian distribution and its higher dimensional counterparts are parameterized by a vector( the mean) and a matrix (the covariance matrix)</p>
<p><span class="math display">\[\begin{bmatrix} x_1\\x_2 \end{bmatrix} = N\Big( \begin{bmatrix} \mu_1\\\mu_2 \end{bmatrix},\begin{bmatrix} \Sigma_{11} &amp; \Sigma_{12}\\\Sigma_{21} &amp; \Sigma_{22} \end{bmatrix} \Big)\]</span></p>
<p>A bi-variate Gaussian can be represented using a simple scatter plot as shown below.</p>
<pre class="r"><code>library(ggplot2)
df = data.frame(x= rnorm(10000),y =rnorm(10000))
ggplot(df,aes(x=x,y=y)) + geom_point()+geom_density_2d()+labs(x=&#39;x1&#39;,y=&#39;x2&#39;)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Note that the bi-variate Gaussian above is constructed using two independent 1D Gaussians, meaning the two variables being plotted here are independent i.e. information about one doesn’t tell us anything about the other. This is further evidenced by the circular, symmetrical distribution of the points.</p>
<p>Such a standard normal bi-variate Gaussian has mean <span class="math inline">\(\mu = \begin{bmatrix} 0\\0 \end{bmatrix}\)</span> and a covariance matrix <span class="math inline">\(\begin{bmatrix} 1 &amp; 0\\0 &amp; 1 \end{bmatrix}\)</span></p>
<p>Note the off-diagonal elements of the covariance matrix are 0.</p>
<p>The plot below shows a bi-variate Gaussian with <span class="math inline">\(\mu = \begin{bmatrix} 0\\0 \end{bmatrix}\)</span> and a covariance matrix <span class="math inline">\(\begin{bmatrix} 1 &amp; 0.7\\0.7 &amp; 1 \end{bmatrix}\)</span>.</p>
<pre class="r"><code>library(mvtnorm)
gaussian_2d &lt;- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,0.7,0.7,1),ncol=2)))
ggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() </code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Here the two random variables are positively correlated and are not independent,because if one is positive, we know that the second is also far more likely to be positive rather than negative</p>
<p>Similarly,the plot below shows a bi-variate Gaussian with <span class="math inline">\(\mu = \begin{bmatrix} 0\\0 \end{bmatrix}\)</span> and a covariance matrix <span class="math inline">\(\begin{bmatrix} 1 &amp; -0.7\\-0.7 &amp; 1 \end{bmatrix}\)</span></p>
<pre class="r"><code>gaussian_2d &lt;- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,-0.7,-0.7,1),ncol=2)))
ggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() </code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Here the two variables are negatively correlated.</p>
<p>Now consider a different visualization of the same data in the figure above.Only 10 of the 10,000
points plotted above is visualized here.</p>
<pre class="r"><code>library(gganimate)
library(tidyr)
library(dplyr)
library(magrittr)

#Select top 10 rows for visualization
viz_data &lt;- head(gaussian_2d,10)
viz_data$set &lt;- as.integer(c(1:10))
#Convert to long format
viz_data_long &lt;- gather(viz_data,key=&#39;datapoint&#39;,value=&#39;y&#39;,-set) %&gt;% arrange(set) %&gt;% 
                    select(-datapoint) %&gt;% 
                      mutate(x = rep(c(1,2),length(unique(viz_data$set)))) %&gt;%
                        select(set,x,y)</code></pre>
<pre class="r"><code>ggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + 
  geom_line(col=&#39;cornflowerblue&#39;)+ transition_time(set)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-6-1.gif" /><!-- --></p>
<p>The 2D Gaussian distribution can be used to model a line between two given points. A Gaussian distribution
with N dimensions can be used to model a line through N different points as shown below.</p>
<p>A 10 D Gaussian with a covariance matrix shown below can be constructed.</p>
<pre class="r"><code>N  &lt;- 10 # dimension of required cov matrix
cov_mat &lt;- diag(N)

vec &lt;- seq(1,0.1,by = -1/N)[2:N]

for(i in 1:(N-1)){
  cov_mat[i,(i+1):N] &lt;- cov_mat[(i+1):N,i] &lt;- vec[1:(N-i)]
}

rownames(cov_mat) &lt;- colnames(cov_mat)&lt;- paste0(&#39;X&#39;,c(1:10))
print(cov_mat)</code></pre>
<pre><code>##      X1  X2  X3  X4  X5  X6  X7  X8  X9 X10
## X1  1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
## X2  0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2
## X3  0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3
## X4  0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4
## X5  0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5
## X6  0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6
## X7  0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7
## X8  0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8
## X9  0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9
## X10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0</code></pre>
<p>The plot below shows a gaussian distribution in 10 dimensions, which can potentially be used to model a line passing through 10 points.</p>
<pre class="r"><code>ggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + 
  geom_line(col=&#39;cornflowerblue&#39;)+ transition_time(set)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-9-1.gif" /><!-- --></p>
<p>This should give you the intuition that using an N - dimensional Gaussian can be used to model a line through N number of points and an infinite dimensional Gaussian can model a continuous curve. This leads to the idea for Gaussian process regression.</p>
<p>Also compare the covariance between any two points in the covariance matrix above and how it manifests in the graph. X1 and X2 have a high covariance, hence the points at 1 and 2 on the x-axis are close to each other along the y-axis. X1 and X10 on the other hand have a low covariance,hence the points 1 and 10 on the x-axis can vary widely.</p>
<p>Conversely if two points lie close to each other on the y-axis, they are likely to have a high covariance whereas if 2 points lie far apart from each other on the y-axis, they are likely to have a low covariance.</p>
</div>
<div id="marginal-distributions-and-conditional-distributions" class="section level2">
<h2>Marginal Distributions and Conditional Distributions</h2>
<p>Given a jointly Gaussian distribution</p>
<p><span class="math display">\[\begin{bmatrix} x_1\\x_2 \end{bmatrix} = N\Big( \begin{bmatrix} \mu_1\\\mu_2 \end{bmatrix},\begin{bmatrix} \Sigma_{11} &amp; \Sigma_{12}\\\Sigma_{21} &amp; \Sigma_{22} \end{bmatrix} \Big)\]</span></p>
<p>according to the <strong>Multivariate Gaussian Theorem</strong>, the marginal distributions of the two component variables are given by.</p>
<p><span class="math display">\[ p(x_1) = N (x_1 | \mu_1.\Sigma_{12}) \]</span></p>
<p><span class="math display">\[ p(x_2) = N (x_2 | \mu_2.\Sigma_{22}) \]</span></p>
<p>The conditional posterior distribution is given by:</p>
<p><span class="math display">\[ p(x_2|x_1) = N (x_2 | \mu_{2|1},\Sigma_{2|1}) \]</span>
where:</p>
<p><span class="math display">\[ \mu_{2|1} = \mu_2 + \Sigma_{21}\Sigma_{11}^{-1} (x_1 - \mu_1) \]</span></p>
<p>and</p>
<p><span class="math display">\[ \Sigma_{2|1} = \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12} \]</span></p>
<p>These expression are critical to do inference with Gaussian Process Regression.</p>
</div>
<div id="gaussian-process-regression." class="section level2">
<h2>Gaussian Process Regression.</h2>
<p>Consider three points shown below which you want to model using a Gaussian distribution.</p>
<pre class="r"><code>plot(data.frame(x = c(1,1.5,3) , f = c(0.2,0.25,0.8)),xlim=c(0,5),ylim = c(0,1),pch=16)
text(x=c(1,1.5,3),y = c(0.25,0.3,0.85),labels=c(&#39;(x1,f1)&#39;,&#39;(x2,f2)&#39;,&#39;(x3,f3)&#39;))</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The three points above can be modeled using a Gaussian distribution of the form below. Note that a zero vector can be used for the mean after standardizing the data.</p>
<p><span class="math display">\[\begin{bmatrix} f_1\\f_2\\f_3 \end{bmatrix} = N\Bigg( \begin{bmatrix} 0\\0\\0 \end{bmatrix},\begin{bmatrix} K_{11} &amp; K_{12} &amp; K_{13}\\K_{21} &amp; K_{22} &amp; K_{23}\\K_{31} &amp; K_{32} &amp; K_{33} \end{bmatrix} \Bigg) = N(\textbf{0},\textbf{K})\]</span></p>
<p>To model these data points using a Gaussian distribution , we need to construct a covariance matrix such that the terms <span class="math inline">\(K_{ij}\)</span> captures the similarity or covariance between points i and j.</p>
<p>The similarity between two points can be captured using kernel functions.Consider a squared exponential kernel(also referred to as a radial basis function kernel) that captures the similarity between two points <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span></p>
<p><span class="math display">\[K_{ij} =  e^{ -\sigma || X_i - X_j ||^2}\]</span></p>
<p><span class="math display">\[K_{ij} = \begin{cases} 
                     0, \text{if}\ ||X_i -X_j|| \to \infty \\
                     1, \text{if}\ X_i = X_j
          \end{cases}\]</span></p>
<p>The covariance matrix for the three points can be calculated as shown below</p>
<pre class="r"><code>library(kernlab)
rbfkernel &lt;- rbfdot(sigma = 0.1) # Set value of hyperparameter
point1 &lt;- c(1)
point2 &lt;- c(1.5)
point3 &lt;- c(3)

K12 &lt;- K21 &lt;- rbfkernel(point1,point2)
K23 &lt;- K32 &lt;- rbfkernel(point2,point3)
K13 &lt;- K31 &lt;-rbfkernel(point1,point3)
K11 &lt;- rbfkernel(point1,point1)
K22 &lt;- rbfkernel(point2,point2)
K33 &lt;- rbfkernel(point3,point3)


K &lt;- matrix(c(K11,K12,K13,K21,K22,K23,K31,K32,K33),byrow=TRUE,nr=3)
print(K)</code></pre>
<pre><code>##           [,1]      [,2]      [,3]
## [1,] 1.0000000 0.9753099 0.6703200
## [2,] 0.9753099 1.0000000 0.7985162
## [3,] 0.6703200 0.7985162 1.0000000</code></pre>
<p>Now that we have learned a model to fit the three points, we want to predict the value of a fourth point <span class="math inline">\(x^*\)</span>, the predicted value for this point <span class="math inline">\(f^*\)</span> can lie anywhere along the red vertical lines.</p>
<pre class="r"><code>plot(data.frame(x = c(1,1.5,3) , f = c(0.2,0.25,0.8)),xlim=c(0,5),ylim = c(0,1),pch=16)
abline(v= 2.5,col=&#39;red&#39;)
text(x=c(1,1.5,3,2.15),y = c(0.25,0.3,0.85,0.5),labels=c(&#39;(x1,f1)&#39;,&#39;(x2,f2)&#39;,&#39;(x3,f3)&#39;,&#39;(x*=2.5,f*)&#39;))</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>In order to capture the relationship between the four points, the four points can be modeled as a 4D Gaussian defined as follows:</p>
<p><span class="math display">\[\begin{bmatrix} \textbf{f}\\f^* \end{bmatrix} = N\Bigg( \begin{bmatrix} \textbf{0}\\0 \end{bmatrix},\begin{bmatrix} \textbf{K} &amp; \textbf{K}_*\\\textbf{K}_*^T &amp; K_{**} \end{bmatrix} \Bigg) \]</span></p>
<p>where:</p>
<p><span class="math display">\[ \textbf{f} = \begin{bmatrix} f_1\\f_2\\f_3 \end{bmatrix}\]</span>
<span class="math display">\[ \textbf{0} = \begin{bmatrix} 0\\0\\0 \end{bmatrix}\]</span>
<span class="math display">\[ \textbf{K} =\begin{bmatrix} K_{11} &amp; K_{12} &amp; K_{13}\\K_{21} &amp; K_{22} &amp; K_{23}\\K_{31} &amp; K_{32} &amp; K_{33} \end{bmatrix}\]</span>
<span class="math display">\[\textbf{K}_* = \begin{bmatrix} K_{1*}\\K_{2*}\\K_{3*} \end{bmatrix}\]</span></p>
<p>This is a joint distribution over <span class="math inline">\(\textbf{f}\)</span> and <span class="math inline">\(f^*\)</span>. Given <span class="math inline">\(\textbf{f}\)</span> which we already know from the three available data points, we can use the <strong>Multivariate Gaussian Theorem</strong> to estimate the distribution over <span class="math inline">\(f_*\)</span>. The mean and variance of this distribution is given by:</p>
<p><span class="math display">\[ \mu^* = \textbf{K}_*^{T}\textbf{K}^{-1}f\]</span></p>
<p><span class="math display">\[ \sigma^* = K_{**} - \textbf{K}_*^{T} \textbf{K}^{-1} \textbf{K}_* \]</span></p>
<p>We can get mean and variance of the distribution over the unknown value at x = 2.5 as follows</p>
<pre class="r"><code>f &lt;- c(0.2,0.25,0.8)

point_star &lt;- c(2.5)

K1star &lt;- Kstar1 &lt;- rbfkernel(point1,point_star)
K2star &lt;- Kstar2 &lt;- rbfkernel(point2,point_star)
K3star &lt;- Kstar3 &lt;-rbfkernel(point3,point_star)
Kstarstar &lt;- rbfkernel(point_star,point_star)

Kstar = c(K1star,K2star,K3star)
Kstar_t = t(Kstar)
K_inv = solve(K)

##The mean##
mu_star = Kstar_t%*%K_inv%*%f

##The  standard deviation##
sigma_star = sqrt(Kstarstar - Kstar_t%*%K_inv%*%Kstar)

print(paste0(&#39;Mean of f* = &#39;,mu_star))</code></pre>
<pre><code>## [1] &quot;Mean of f* = 0.575736376567534&quot;</code></pre>
<pre class="r"><code>print(paste0(&#39;Standard deviation of f* = &#39;,sigma_star))</code></pre>
<pre><code>## [1] &quot;Standard deviation of f* = 0.0264409370474204&quot;</code></pre>
<p>The prediction is shown below in green using an error bar.</p>
<pre class="r"><code>plot(data.frame(x = c(1,1.5,3,point_star) , f = c(0.2,0.25,0.8,mu_star)),xlim=c(0,5),ylim = c(0,1),pch=16, col = c(rep(&#39;black&#39;,3),&#39;green&#39;))
arrows(point_star,mu_star - 1.96*sigma_star,point_star,mu_star + 1.96*sigma_star,length = 0.05,angle=90,
       code=3,col=&#39;green&#39;)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Similarly the prediction of any point along the x-axis can be found. Below the predictions for a sequence of points along the x-axis have been generated and plotted along with the their 95% confidence intervals.</p>
<p>For any input value, the Gaussian process model gives us a mean and a variance for the prediction.It is in effect a function.</p>
<pre class="r"><code>predict &lt;- function(input){
  
  point_star &lt;- input

  K1star &lt;- Kstar1 &lt;- rbfkernel(point1,point_star)
  K2star &lt;- Kstar2 &lt;- rbfkernel(point2,point_star)
  K3star &lt;- Kstar3 &lt;-rbfkernel(point3,point_star)
  Kstarstar &lt;- rbfkernel(point_star,point_star)

  Kstar = c(K1star,K2star,K3star)
  Kstar_t = t(Kstar)
  K_inv = solve(K)

  ##The mean##
  mu_star = Kstar_t%*%K_inv%*%f

  ##The standard deviation##
  sigma_star = tryCatch(sqrt(Kstarstar - Kstar_t%*%K_inv%*%Kstar),
                        warning = function(e){0})
  
  return(c(mu_star,sigma_star))
}

##Vector of points to generate predictions on
x &lt;- seq(0,5,by=0.1)

predictions &lt;- sapply(x,predict)
mean &lt;- predictions[1,]
#lower bound
lb &lt;- predictions[1,] - 1.96*predictions[2,]
#upper bound
ub &lt;-predictions[1,] + 1.96*predictions[2,]
#Bind together in a dataframe
preds &lt;- data.frame(cbind(x,mean,lb,ub))
#Inputs
inputs &lt;-  data.frame(x=c(point1,point2,point3),f=f)


ggplot(preds,aes(x=x,y=mean)) + 
  geom_ribbon(aes(x=x,ymin=lb,ymax=ub),fill=&#39;grey70&#39;)+
  geom_line(col=&#39;green&#39;)+
  geom_point(data=inputs,mapping=aes(x=x,y=f))+labs(y=&#39;y&#39;)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>We can clearly see that the uncertainty near the training data points is low, as we move away from the training data points, the level of uncertainty increases. This aligns very well with our intuition.</p>
</div>
<div id="gaussian-processes" class="section level2">
<h2>Gaussian Processes</h2>
<p>The Gaussian process is thus the generalization of a multi-variate normal distribution to infinitely many variables.It can be thought of as a Gaussian distribution over functions( thinking of functions as infinitely long vectors containing the value of the function at every input)</p>
<p>The choice of a kernel function encodes a prior belief that the data points that lie close to each other should result in similar function outputs.The value of the hyper parameter sigma encodes how smooth the functions are.</p>
<p>Consider a sequence of test points from -5 to 5. The GP prior, or the distribution over the possible space of functions when no training data is available is as follows. Only three functions have been sampled here.</p>
<pre class="r"><code>x_vec &lt;- seq(-5,5,by=0.1)
rbfkernel &lt;- rbfdot(sigma = 1) # Set value of hyperparameter
cov_mat &lt;- kernelMatrix(rbfkernel,x_vec,x_vec)

gp_prior &lt;- data.frame(t(rmvnorm(5,mean= rep(0,length(x_vec)),sigma = cov_mat)))


matplot(x=x_vec,y = gp_prior,lty=1,type=&#39;l&#39;,xlab =&#39;x&#39;,ylab = &#39;y&#39;,col=c(1:10))</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Assume the data generating process is a sine function, and the value of the sine function at three points
1,1.5 and 3 were observed as shown below.</p>
<pre class="r"><code>plot(x=x_vec,y=sin(x_vec),type=&#39;l&#39;,xlab=&#39;x&#39;,ylab=&#39;y&#39;)
points(x=c(0.5,2,3),y=sin(c(0.5,2,3)),pch =16)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>train_x &lt;- c(0.5,2,3)
test_x &lt;- x_vec
f &lt;- sin(train_x)
K &lt;- kernelMatrix(rbfkernel,train_x,train_x)
K_star &lt;- kernelMatrix(rbfkernel,train_x,test_x)
K_starstar &lt;- kernelMatrix(rbfkernel,test_x,test_x)
K_inv &lt;- solve(K)</code></pre>
<p>The mean and variance of the test points are calculated as follows. Note that there are more efficient ways of doing the linear algebra calculations shown below, specifically be reducing the matrix inverse to linear system of equations. References for doing this have been provided in the appendix.</p>
<pre class="r"><code>mu_star &lt;- t(K_star)%*%K_inv%*%f
Sigma_star &lt;- K_starstar - t(K_star)%*%K_inv%*%K_star</code></pre>
<p>Sampling from this GP posterior produces the following posterior distribution of functions.</p>
<pre class="r"><code>gp_posterior &lt;- data.frame(t(rmvnorm(5,mean= rep(0,length(mu_star)),sigma = Sigma_star)) + as.vector(mu_star))
matplot(x=test_x,y = gp_posterior,lty=1,type=&#39;l&#39;,xlab =&#39;x&#39;,ylab = &#39;y&#39;,col=c(1:10))
lines(x=x_vec,y=sin(x_vec),lty=2,lwd=2)
points(x=train_x,y=f,pch=16)</code></pre>
<p><img src="/post/2019-08-11-introduction-to-gaussian-processes_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>It can be seen that the presence of data squishes all three functions to a single point.The variance across the three functions goes to 0 at these points and increases with distance from these points.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Gaussian Processes provide an interesting alternative to generalized linear models. It comes with a minimal set of assumptions and provides a quantitative estimate of uncertainty in predictions. This also makes it applicable in areas like active learning.</p>
<p>Gaussian Process are very computationally intensive and it’s application to large data sets have therefore been limited. Research in Gaussian Processes have been focused on making these computations more tractable.</p>
</div>
<div id="references-and-additional-resources" class="section level2">
<h2>References and Additional Resources</h2>
<ol style="list-style-type: decimal">
<li><p>Almost all the content in this write up comes from Nando de Freitas’ <a href="https://www.youtube.com/watch?v=4vGiHC35j9s">UBC lecture</a></p></li>
<li><p>A second lecture I referenced is available <a href="//www.youtube.com/watch?v=92-98SYOdlY&amp;t=2216s">here</a></p></li>
<li><p><a href="https://gpytorch.ai/">GPyTorch</a>: An open source project leveraging GPs for machine learning using PyTorch.</p></li>
<li><p><a href="https://cran.r-project.org/web/packages/kernlab/vignettes/kernlab.pdf">kernlab</a>: A package in R for Kernel Methods</p></li>
<li><p>A high quality intro to GPs from <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">distill.pub</a></p></li>
<li><p>Largely the same content in <a href="http://katbailey.github.io/post/gaussian-processes-for-dummies/">python</a></p></li>
</ol>
</div>
